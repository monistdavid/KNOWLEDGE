1. what cause the over-fitting in machine learning, is it also a case in human's learning process?
2. when we train a neuron network, we decrease the entropy loss to a certain level that the model could do well on 
   both the test set and training set. So the purpose is to keep the randomness to an appropriately level. However, 
   there shall be different randomness level applied to different situation.
3. we judge the good-or-bad of the prompting by seeing the output of the model, which is subjective. What if the model
   itself knows the good-or-bad of the prompting and could give advices on how to improve the given prompt. 
4. if we learn how all users in github name and arrange their folders, files, etc. Can we make a knowledge graph that 
   could automatically arrange the knowledge approximately?
5. AI could be the UI tester?
6. If no one is naming those weird terms, we could easily understand most part of the papers.
7. Gravity seems influence the physical forms of us in earth, what could gravity influence the neuron network?