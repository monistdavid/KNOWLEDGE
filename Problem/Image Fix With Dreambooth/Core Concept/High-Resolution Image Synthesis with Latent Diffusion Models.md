Link
===============
<p>

High-Resolution Image Synthesis with Latent Diffusion Models
https://arxiv.org/pdf/2112.10752.pdf

</p>

Summary
===============


Questions and Thoughts Based on little RESEARCH
===============
    1. Mode collapse problem. Similar to a lot other neural networks problems. It tends to focus to much on the 
    result only, but rarely on the process. 
            Discussing about the difference between traditional machine learning and deep learning. The machine learning
        try to analysis the structured data and identify the pattern of the data. While the deep learning
        is good at analysising the unstructured data and automatically analysis the hidden features of the data.
            I would say deep learning is a much clever student than the machine learning. The deep learning, 
        refering the neural network is able to learn more from the data by itself, comparing with traditional machine learning.
        However, the model itself is getting more clever, the teaching method is not getting better. For both
        machine learning and deep learning, we are using the same way to teach them, it's just deep learning
        have more self-teaching skill compared with machine learning. Being teachers as us, we didn't have
        a better way to teach them instead of just putting all the data to the models. Just like we just give all
        the materials to those clever students and ask them to learn by themselves.
            Another thing to think about is, shall we really interfere the learning process of the clever students
        (model)?
            In reverse mode-covering is saying the model is able to cover wide range of data distributions.
        
        I feel like the neural network is a more clever student compared with most of the humans, so it should
        come up with ways to study by itself.
    2. 


        

Creativity
==============
