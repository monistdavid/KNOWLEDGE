Link
===============
<p>

DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
https://arxiv.org/abs/1910.01108

</p>

Summary
===============

Questions and Thoughts Based on little RESEARCH
===============

    1. But some of these "near-zero" probabilities are larger than others and reflect, in part, the
    generalization capabilities of the model and how well it will perform on the test set.
            This is more like a comparision between the result and the process. You can get most
        of the result correct doesn't mean you have a good understanding of the problem. Just like
        a student who have done thousands of math exams and know how to take math exams doesn't
        mean he is good at math. With the wrong focusing on the training purpose, we might get
        the wrong model in the end. We trained the model just for the final exam, the model does
        well on the final exam but never really understand the knowledge. So, how to train a model
        that's goal is to understand the knowledge?
        Personally as a human being, the one I understand the knowledge is to ask questions and search
        the accuracte knowledge to answer the questions. searching I mean online, offline, talk to professors, etc.
    2.  variations on the last dimension of the tensor (hidden size dimension) have
        a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other
        factors like the number of layers. Thus we focus on reducing the number of layers.
        So the problem of how to make it smaller is a problem itself. 




Creativity
==============
    1. train a model that could ask good questions, and search the knowledge to answer the questions.