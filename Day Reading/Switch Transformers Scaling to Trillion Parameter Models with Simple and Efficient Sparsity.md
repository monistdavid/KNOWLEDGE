Link
===============
<p>

Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
https://arxiv.org/pdf/2101.03961.pdf

</p>

Summary
===============

        Based on the Mixture of Expert, the switch transofmers use one expert instead of multiple
    experts. It makes sure the training process takes more parameter counts and faster training speed 
    but remaining the same training resource cost. 

Questions and Thoughts Based on little RESEARCH
===============

    1. is it possible to have the model predict the complexity of a task and decide how many resources (parameters)
        could be used on this task?

Creativity
==============