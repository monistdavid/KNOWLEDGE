Link
===============
<p>

LLaMA: Open and Efficient Foundation Language Models
https://arxiv.org/pdf/2302.13971.pdf

</p>

Summary
===============

        This paper published a pre-trained language model called LLaMA, which is a large language model
    that referenced from Chinchilla scaling laws. So the model with 68B parameters outperform GPT-3.

Questions and Thoughts Based on little RESEARCH
===============

    1. We both know the increasing of model size could increase the performance of the model, and 
    we also know that increasing of the training token size approximately could also increase the
    performance of the model. Then what is the importance of the data quality, how good influence
    is on the performance of the model for those bad data?

Creativity
==============

