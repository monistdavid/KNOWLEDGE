Link
===============
<p>

LLaMA: Open and Efficient Foundation Language Models
https://arxiv.org/pdf/2302.13971.pdf

</p>

Summary
===============
        This paper published a pre-trained language model called LLaMA, which is a large language model
    that referenced from Chinchilla scaling laws. So the model with 68B parameters outperform GPT-3.