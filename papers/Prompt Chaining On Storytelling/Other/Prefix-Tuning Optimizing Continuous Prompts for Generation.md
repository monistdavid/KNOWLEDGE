Link
===============
<p>

Prefix-Tuning: Optimizing Continuous Prompts for Generation
https://arxiv.org/pdf/2101.00190.pdf

</p>

Summary
===============

        The paper proposed a method to fine-tune the pre-trained model by maintaining the parameter
    of the original model, but add extra continuous tokens as input to the model. By fine-tuning these new
    added tokens, the model mimic the prompt engineering by using these tokens to steer the model to 
    perfore specific task. This new method is called prefix tuning.
