Link
===============
<p>

Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers
https://arxiv.org/pdf/2208.12242.pdf

</p>

Summary
===============



Questions and Thoughts Based on little RESEARCH
===============
        1. if we believe a smarter student could learn things faster, why a large language model could not learn domain 
    knowledge fast? Could not be fine-tuned easily? 
        By saying easily, we should mean it learns faster and better with a relatively small dataset. It actually takes
    a much larger computational cost. So by focusing on a spcefic task, but relatively easy task, there is no need to 
    use extremely large language models. Smaller models like GPT-J should be good enough. 


Creativity
==============
