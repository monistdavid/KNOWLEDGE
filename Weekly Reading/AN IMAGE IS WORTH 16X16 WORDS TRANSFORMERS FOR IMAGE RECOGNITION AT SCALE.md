Link
===============
<p>

AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE
https://arxiv.org/pdf/2010.11929.pdf

</p>

Summary
===============

Questions and Thoughts Based on little RESEARCH
===============
    1. Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and 
        locality, and therefore do not generalize well when trained on insufficient amounts of data.
        Seems like Transformers are students who have more potential than CNNs.
        Smart students learn faster, but can we make them learn better even with less data?
        We shall say Transformers spend more time on understanding the concept while CNN spend more time on 
        remembering the knowledge. So for a small exam, CNN does better because they could just remember the answer, 
        Transformers could do better in life because they could use one concept to explain a lot of stuff?

    2. pay attention on global as well as local can improve the model performance.
        How about pay more attention on basic and foundamental instead of advanced concept.

Creativity
==============
        
